common:
  prefix: /api/o11y/insight
  DB:
    URL: mc-observability-maria
    USERNAME: mc-agent
    PASSWORD: mc-agent
    DATABASE: mc_observability
  InfluxDB:
    HOST: mc-observability-influx
    PORT: 8086
    USERNAME: mc-agent
    PASSWORD: mc-agent
    DATABASE: insight
    POLICY: autogen
  MC-O11Y:
    URL: mc-observability-manager
    PORT: 18080

anomaly:
  target_types:
    types:
      - vm
      - mci
  measurements:
    types:
      - cpu
      - mem
  measurement_fields:
    - measurement: cpu
      fields:
        - field_key: usage_idle
          unit: percent
    - measurement: mem
      fields:
        - field_key: used_percent
          unit: percent
  execution_intervals:
    intervals:
      - 5m
      - 10m
      - 30m
  rrcf:
    num_trees: 10
    shingle_ratio: 0.01
    tree_size: 1024
    anomaly_range_size: 2.5

prediction:
  target_types:
    types:
      - vm
      - mci
  measurements:
    types:
      - cpu
      - mem
      - disk
      - system
  measurement_fields:
    - measurement: cpu
      fields:
        - field_key: usage_idle
          unit: percent
    - measurement: disk
      fields:
        - field_key: used_percent
          unit: percent
    - measurement: mem
      fields:
        - field_key: used_percent
          unit: percent
    - measurement: system
      fields:
        - field_key: load1
          unit: percent

  prediction_ranges:
    min: 1h
    max: 2160h
  prediction_history:
    default_range: 168
  prophet:
    PROPHET_CPS: 10.0
    PROPHET_SPS: 10.0
    PROPHET_HPS: 0.05
    PROPHET_SM: additive
    REMOVE_COLUMNS:
      - trend
      - yhat_lower
      - yhat_upper
      - trend_lower
      - trend_upper
      - additive_terms
      - additive_terms_lower
      - additive_terms_upper
      - daily
      - daily_lower
      - daily_upper
      - weekly
      - weekly_lower
      - weekly_upper
      - multiplicative_terms
      - multiplicative_terms_lower
      - multiplicative_terms_upper

llm:
  model:
    - provider: ollama
      model_name:
        - llama3.1:8b
        - mistral:7b
    - provider: openai
      model_name:
        - gpt-5
        - gpt-5-mini
        - gpt-5-nano
    - provider: google
      model_name:
        - gemini-2.5-flash
        - gemini-2.5-pro
    - provider: anthropic
      model_name:
        - claude-3-5-haiku-latest
        - claude-3-7-sonnet-latest
        - claude-sonnet-4-0
        - claude-opus-4-0
        - claude-opus-4-1
  mcp:
    mcp_grafana_url: http://mc-observability-mcp-grafana:8000/sse
    mcp_mariadb_url: http://mc-observability-mcp-mariadb:8001/sse
    mcp_influxdb_url: http://mc-observability-mcp-influxdb:8002/sse
  chat_summarization:
    max_tokens_before_summary: 5000
    summary_prompt: |
      Please generate a **concise and structured summary** of the entire conversation in bullet points, 
      covering all messages from the beginning without omitting important details.
      
      ### Guidelines
      1. **Intentions**: State the user's goals, questions, or issues.  
      2. **Technical Details**: Keep key info (e.g., datasource UID, configs, system/service names).  
      3. **Context**: Note links with previous results or actions.  
      4. **Clarity**: Exclude greetings, repetition, or generic advice.  
      5. **Progress/Next Steps**: Mention ongoing work or pending tasks.  
      6. **Formatting**: Use bullet points and clear sections.
      
      ### Output Style
      - Factual, neutral, professional tone  
      - Concise but complete  
      - Easy to scan for key insights    

log_analysis:
  system_prompt_first: |
    <system_prompt>
    You are an agent - Log Analysis Specialist.
    Time: {current_time}
  system_prompt_default: |
    Continue the conversation as the Log analysis AI assistant for the MC-Observability platform.
    No guessing, answer based only on MCP tool results.
    Current time: {current_time}

alarm_analysis:
  system_prompt_first: |
    <system_prompt>
    You are an agent - System Alarm Investigator.
    You MUST search MariaDB first: mc_observability -> mc_o11y_trigger_history to identify relevant alarms/incidents, THEN use InfluxDB secondarily (mc-observability DB) to analyze time-series metrics corresponding to those incidents. Do NOT search any other sources.
    Your primary directive is to base all findings strictly on real data retrieved via the MCP tools. You MUST first consult MariaDB, and only after extracting necessary context (e.g., time windows, affected hosts, resources) may you construct appropriate InfluxQL queries for detailed analysis.
    You are strictly forbidden from inventing data, assuming data exists without verification, or providing speculative answers.
    
    Your Mission:
    
    1. Alarm & Context Discovery (MariaDB first): Query mc_o11y_trigger_history to determine timeframe, target (vm_id/host), and event types.
    2. Metric Analysis (InfluxDB second): Build precise InfluxQL using the discovered context to validate anomalies and quantify impact.
    3. Actionable Recommendations: Provide concrete next steps for operators.
    
    Response format (use these three sections):
    - Analysis Result
    - Potential Causes
    - Actionable Recommendations
    </system_prompt>
    
    <mcp_introduction>
    You can use MariaDB and InfluxDB MCP.
    Priority policy:
    - 1st: MariaDB (mc_observability.mc_o11y_trigger_history).
    - 2nd: InfluxDB (mc-observability) using context derived from MariaDB.
    Use tools appropriately and return outputs faithfully.
    </mcp_introduction>
    
    Time: {current_time}
  system_prompt_default: |
    Continue the conversation as the alarm analysis AI assistant for the MC-Observability platform.
    Priority Policy: Check events/duration/target in priority MariaDB (mc_observability.mc_o11y_trigger_history) and analyze metrics in the context in priority 2 in fluxDB (mc-observability).
    No guessing, answer based only on MCP tool results.
    Current time: {current_time}